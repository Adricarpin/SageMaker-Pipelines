{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Pipelines: Deploying End-to-End Machine learning Pipelines in the Cloud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to deploy an End-To-End Machine Learning Pipeline with Amazon SageMaker. We will work with [Adult Census Income](https://www.kaggle.com/uciml/adult-census-income) Dataset. We will use 'income', a binary variable that explains if a person earns more than 50k or not, as the target variable. For the training step, we will use an image of XGBoost. \n",
    "\n",
    "In order to replicate the results in your SageMaker Studio, please clone the repository (if you don't know how to do it look for it in the README.md). \n",
    "\n",
    "Once you have this notebook and the data in your SageMaker Studio, is time to start!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we set the required dependencies to use SageMaker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker_session.default_bucket() # the default S3 bucket where you will store everything that cames from the pipeline\n",
    "model_package_group_name = f\"AdultModelPackageGroupName\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we will copy the data from the local path (the data from the side bar) to the  default S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \"data/adult.csv\" # local path where you have the data\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "base_uri = f\"s3://{default_bucket}/adult\"\n",
    "\n",
    "# This line copies your data in the local path to a default S3 bucket\n",
    "input_data_uri = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=local_path,\n",
    "    desired_s3_uri=base_uri,\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will specify some parameters that will be useful when setting the pipeline. Since we will manage a good amount of variables, I think is a good practice to use the Find bar (ctrl+F) to see when a variable will be used or where it came from.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "\n",
    "\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "\n",
    "\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\", default_value=\"ml.m5.xlarge\"\n",
    ")\n",
    "\n",
    "training_instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=\"ml.m5.xlarge\")\n",
    "\n",
    "\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\"\n",
    ")\n",
    "\n",
    "input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=input_data_uri,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now its time to go with the first step. We will create a preprocessing script and store it in a folder called 'adult'. Then we will create a ```SKLearnProcessor``` instance and specify some dependencies to run the Preprocessing Step. Finally, we will pass the code and the Processor to a ```ProcessingStep```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p adult # create a folder called 'adult'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting adult/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile adult/preprocessing.py \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    base_dir = \"/opt/ml/processing\" # this path will make more sense once you build the ProcessingStep\n",
    "    # read data\n",
    "    df = pd.read_csv(f\"{base_dir}/input/adult.csv\", sep=\",\", \n",
    "                     error_bad_lines=False, engine='python') # just to avoid an error\n",
    "\n",
    "    # target variable to binary\n",
    "    df['income'].replace(['<=50K','>50K'],[0,1], inplace=True) \n",
    "\n",
    "    # drop useless variables\n",
    "    df = df.drop('fnlwgt', axis=1)\n",
    "    df = df.drop('education.num', axis=1)\n",
    "\n",
    "    # drop rows with missing data\n",
    "    df = df.loc[ (df['workclass'] != '?') & (df['occupation'] != '?') & (df['native.country']!= '?')]\n",
    "\n",
    "    # split data into dependent and independent variables\n",
    "    X = df.drop('income', axis=1)\n",
    "    y = df['income']\n",
    "\n",
    "    # split dependent variables into continous and categorical variables\n",
    "    X_continous  = X[['age', 'capital.gain', 'capital.loss', 'hours.per.week']]\n",
    "\n",
    "    X_categorical = X[['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race',\n",
    "                    'sex', 'native.country']]\n",
    "\n",
    "\n",
    "    # One hot encoding\n",
    "    X_encoded = pd.get_dummies(X_categorical)\n",
    "\n",
    "    # Concatenate both continous and encoded sets:\n",
    "    X = pd.concat([X_continous, X_encoded],axis=1)\n",
    "\n",
    "    y = y.to_numpy().reshape(y.shape[0],1)\n",
    "    X = X.to_numpy()\n",
    "\n",
    "    # create the processed dataset\n",
    "    dataset = np.concatenate((y, X), axis=1)\n",
    "\n",
    "    np.random.shuffle(dataset)\n",
    "    \n",
    "    # Split into train validation and test datasets\n",
    "    train, validation, test = np.split(dataset, [int(.6*len(dataset)), int(.7*len(dataset))])\n",
    "\n",
    "    # Save the data\n",
    "    pd.DataFrame(train).to_csv(f\"{base_dir}/train/train.csv\", header=False, index=False)\n",
    "    pd.DataFrame(validation).to_csv(f\"{base_dir}/validation/validation.csv\", header=False, index=False)\n",
    "    pd.DataFrame(test).to_csv(f\"{base_dir}/test/test.csv\", header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a SKLearn processor instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "\n",
    "framework_version = \"0.23-1\"\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"sklearn-adult-process\",\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we pass the processor and the code into a ```ProcessingStep```. We also specify the inputs and outputs paths. \n",
    "NOTE: '/opt/ml/processing' is just a default path in the processing container. You can take a quick look  [here](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html) if you want more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"adultProcess\",\n",
    "    processor=sklearn_processor, # SKLearnProcessor\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),\n",
    "    ], # copies the data from the S3 bucket to the container\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "    ], # You store the output in s3 (example: sagemaker-us-east-<>/<base_job_name>/output/train/train.csv)\n",
    "    code=\"adult/preprocessing.py\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we are going to import a build-in image of XGBoost and use it as the estimator for the ```TrainingStep``` class. We also need to set the hyperparameters. When building the ```TrainingStep``` we will pass the estimator and the inputs ('train.csv' and 'test.csv')\n",
    "\n",
    "\n",
    "\n",
    "NOTE: You can also perform [automatic model tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html) with SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# Set the model path in the S3 bucket\n",
    "model_path = f\"s3://{default_bucket}/adultTrain\"\n",
    "\n",
    "# Retrieve the XGBoost image from the SageMaker repository\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.0-1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "\n",
    "# Set the XGBoost image as an estimator\n",
    "xgb_train = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    output_path=model_path,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "\n",
    "# Set the hyperparameters \n",
    "xgb_train.set_hyperparameters(\n",
    "    objective=\"binary:logistic\", \n",
    "    num_round=50, \n",
    "    max_depth=5, \n",
    "    subsample=1, \n",
    "    silent=0,\n",
    "    eval_metric=\"logloss\",\n",
    "    eta=0.3, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our estimator, we can pass it to the ```TrainingStep``` class. We also specify the data inputs ('train.csv' and 'validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name=\"adultTrain\",\n",
    "    estimator=xgb_train,\n",
    "    # Data inputs in this step are the outputs in the previous step ('train.csv' and 'validation.csv') \n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",    \n",
    "        ),\n",
    "        \n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"validation\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step first we develop an evaluation script. Then we create a Processor intance with ```ScriptProcessor``` and a property file with ```PropertyFile```. Finally we develop the ```ProcessingStep```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting adult/evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile adult/evaluation.py\n",
    "\n",
    "import json\n",
    "import pathlib\n",
    "import pickle\n",
    "import tarfile\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # import the model\n",
    "    model_path = f\"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "    \n",
    "    model = pickle.load(open(\"xgboost-model\", \"rb\"))\n",
    "\n",
    "    # import test set\n",
    "    test_path = \"/opt/ml/processing/test/test.csv\"\n",
    "    df = pd.read_csv(test_path, header=None)\n",
    "    \n",
    "\n",
    "    # Split into dependent and independent variables\n",
    "    y_test = df.iloc[:, 0].to_numpy()\n",
    "    df.drop(df.columns[0], axis=1, inplace=True)\n",
    "    X_test = xgboost.DMatrix(df.values)\n",
    "    \n",
    "    # make predictions\n",
    "    predictions = model.predict(X_test)\n",
    "    predictions = np.where(predictions > 0.5, 1, 0 )\n",
    " \n",
    "    # compute accuracy\n",
    "    acc = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    # Create a json file with the metrics results\n",
    "    report_dict = {\n",
    "        \"metrics\": {\n",
    "            \"accuracy\": {\n",
    "                \"value\": acc\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    # Set the output directory\n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save the json file with the metrics result\n",
    "    evaluation_path = f\"{output_dir}/evaluation.json\"\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        f.write(json.dumps(report_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is setting the processor instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "\n",
    "script_eval = ScriptProcessor(\n",
    "    image_uri=image_uri, # The XGBoost image\n",
    "    command=[\"python3\"], # We run python3 inside the container\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    base_job_name=\"script-adult-eval\",\n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before defining the ```ProcessingStep``` we are going to define a ```PropertyFile```. You use property files to store information from the output of a processing step. This is particularly useful when analyzing the results of a processing step to decide how a conditional step should be executed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\", output_name=\"evaluation\", path=\"evaluation.json\"\n",
    ")\n",
    "\n",
    "# The path parameter is the name of the JSON file that the property file is saved to.\n",
    "# output_name must match the output_name of the ProcessingOutput that you define in your processing step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define the ```ProcessingStep```. We pass the ```ScriptProcessor```, the inputs ('model.tar.gz' and 'test.csv'), the ```PropertyFile``` and the evaluation.py script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_eval = ProcessingStep(\n",
    "    name=\"adultEval\",\n",
    "    processor=script_eval,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts, # S3 path of model.tar.gz\n",
    "            destination=\"/opt/ml/processing/model\", # destination in the processing container\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=step_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri, # S3 path to test.csv\n",
    "            destination=\"/opt/ml/processing/test\", # destination in the processing container\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "    ], # the output is the 'evaluation.json' file\n",
    "    code=\"adult/evaluation.py\",\n",
    "    property_files=[evaluation_report],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model passes the condition step (defined below), we create a SageMaker Model. A SageMaker Model is an instance that can be deployed to an Endpoint. First we create a model with the```Model``` class. We pass the XGBoost image and the 'model.tar.gz' file to it. Then we create an input with the ```CreateModelInput``` class. Finally we pass both of them to the ```CreateModelStep```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "\n",
    "model = Model(\n",
    "    image_uri=image_uri, # the XGBoost image\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts, # The S3 location of the 'model.tar.gz' file\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import CreateModelInput\n",
    "\n",
    "\n",
    "inputs = CreateModelInput(\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    accelerator_type=\"ml.eia1.medium\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "\n",
    "\n",
    "step_create_model = CreateModelStep(\n",
    "    name=\"adultCreateModel\",\n",
    "    model=model,\n",
    "    inputs=inputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register Model Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We register the model if it passes a condition step (defined below). For the Register Model Step, first we set the model metrics with the ```ModelMetrics``` class, passing the 'evaluation.json' file to it. Then we create the Register Model Step with the ```RegisterModel``` class. We pass the estimator, the 'model.tar.gz' file and the model metrics object to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=\"{}/evaluation.json\".format(\n",
    "            step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "        ),\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")\n",
    "step_register = RegisterModel(\n",
    "    name=\"adultRegisterModel\",\n",
    "    estimator=xgb_train,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts, # The S3 uri to the 'model.tar.gz' file\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Condition step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Condition Step, we will create a condition with the ```ConditionGreaterThanOrEqualTo``` class, defining a threshold and the accuracy value. Then we will pass the Condition object to the ```ConditionStep```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The class JsonGet has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import (\n",
    "    ConditionStep,\n",
    "    JsonGet,\n",
    ")\n",
    "\n",
    "\n",
    "condition = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step=step_eval,\n",
    "        property_file=evaluation_report, # we pass the property file object\n",
    "        json_path=\"metrics.accuracy.value\",\n",
    "    ),\n",
    "    right=0.8, # the threshold\n",
    ")\n",
    "\n",
    "step_cond = ConditionStep(\n",
    "    name=\"adultAccCond\",\n",
    "    conditions=[condition],\n",
    "    if_steps=[step_register, step_create_model], # if the condition is true we execute the Register Model and Create Model steps\n",
    "    else_steps=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a pipeline with the ```Pipeline``` class.  We have to pass the parameters that we use in the steps (the ones defined at the beginning of the notebook), and the steps of the pipeline. The create model step and register model step are triggered if the condition from the condition step is true, so we don't need to specify them in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "\n",
    "pipeline_name = f\"adultPipeline\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        processing_instance_count,\n",
    "        training_instance_type,\n",
    "        model_approval_status,\n",
    "        input_data,\n",
    "    ],\n",
    "    steps=[step_process, step_train, step_eval, step_cond]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we create a pipeline we can now [run it](https://docs.aws.amazon.com/sagemaker/latest/dg/run-pipeline.html). First we submit the pipeline definition to the SageMaker Pipelines service to create a pipeline if it doesn't exist, or update the pipeline if it does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the pipeline, we can just start it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.wait() # wait untill the process ends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the pipeline process is completed, we can check the output metrics from the evaluation step (the 'evaluation.json' file).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metrics': {'accuracy': {'value': 0.8700408884959664}}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "evaluation_json = sagemaker.s3.S3Downloader.read_file(\n",
    "    \"{}/evaluation.json\".format(\n",
    "        step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "    )\n",
    ")\n",
    "pprint(json.loads(evaluation_json))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also see all the steps in the pipeline as well as their inputs and outputs with the following code. I won't run it just in order to not share my account information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "\n",
    "viz = LineageTableVisualizer(sagemaker.session.Session())\n",
    "for execution_step in reversed(execution.list_steps()):\n",
    "    print(execution_step)\n",
    "    display(viz.show(pipeline_execution_step=execution_step))\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to check the different Jobs created, you can go to Amazon Sagemaker and look at them in the sidebar. It should be a Training Job in the Training section, three Processing Jobs in the Processing section and a model in the Inference section."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
